## Technical Approach and Evolution
I began with a frequency-based technique, analyzing letter occurrences across the training dictionary to prioritize common letters. This initial approach yielded a modest 45% success rate, as it struggled to adapt to the test set’s unique word distribution. Recognizing its limitations, I shifted to an entropy-based method, calculating the information gain of each unguessed letter based on possible word patterns. This move significantly boosted performance, pushing success rates to 80–85% in controlled simulations by efficiently narrowing down candidates.

To build upon this, I introduced a hybrid strategy. I enhanced the initial guess sequence with a curated list of high-frequency letters to establish a strong early-game foundation. I then weighted the entropy model with vowel proportion analysis, inspired by character-level prediction concepts like those in the CANINE model, though I did not implement CANINE itself. This adaptation improved adaptability to word structures, particularly for longer words (≥7 letters). For cases where entropy weakened—typically after 6 guesses—I reverted to a frequency-based fallback, ensuring consistent progress. I also pre-organized the dictionary by word length to optimize filtering, reducing computational overhead as the game progressed.

## Efficiency and Performance
The solution’s efficiency arises from its structured dictionary management and adaptive guessing logic. Pre-sorting by length enabled rapid candidate reduction, while the entropy-weighted approach, adjusted by vowel count, maximized information gain per guess. This combination drove practice success rates from 45% to 80–85%, demonstrating a marked improvement over the baseline and highlighting the effectiveness of the hybrid technique.

## Challenges Encountered
Developing this algorithm was challenging, particularly due to the disjoint nature of the test set, which initially undermined the frequency-based approach. Finding a reliable base was difficult, as early attempts resulted in erratic performance, with success rates hovering around 45% and some games exhausting tries within 5–6 guesses. The transition to entropy helped, but adapting it to the test set’s variability required extensive tuning. Additionally, ensuring compatibility with the API’s rate limits and handling potential server inconsistencies added complexity to the process.

## Rationale for the Approach
I started with a frequency-based method due to its simplicity and reliance on the training dictionary’s patterns. However, its poor generalization to the test set prompted a shift to entropy, which proved more effective in simulations. The hybrid approach emerged as I sought to combine the strengths of both—early coverage from frequency and adaptive precision from entropy—while incorporating vowel weighting to mimic character-level insights without external models. This strategy was chosen for its balance of computational feasibility and adaptability, leveraging the dictionary’s structure while addressing the test set’s unpredictability. The focus on longer words aligns with the technique’s strength in pattern recognition as more letters are revealed.

## Conclusion
This submission represents my technical journey to create a robust Hangman solver, achieving practice success rates of 45% to 80–85%. The evolution from frequency to a hybrid entropy-weighted model, built upon a challenging foundation, positions it as a strong candidate for the 1,000-game recorded evaluation.
